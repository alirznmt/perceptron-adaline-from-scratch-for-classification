{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b47599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import erf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63919f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax2(x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1)\n",
    "\n",
    "class perceptron():\n",
    "    def __init__(self ,W ,bias , learning_rate = 0.1 , theta = 0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.theta = theta\n",
    "        self.W = W\n",
    "        self.bias = bias \n",
    "    \n",
    "    def set_weight(self , weight):\n",
    "        self.W  = weight\n",
    "        \n",
    "    def set_theta(self ,theta):\n",
    "        self.theta = theta\n",
    "        \n",
    "    def set_learningrate(self , lr):\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "    def activation(self , Y):\n",
    "        res = np.ones_like(Y)\n",
    "        res[Y > self.theta] = 1  \n",
    "           \n",
    "        res[(Y > -self.theta) & (Y < self.theta)] = 0  \n",
    "             \n",
    "        res[Y < -self.theta] = -1\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self , X , y ):\n",
    "        w = self.W\n",
    "        b = self.bias \n",
    "        b = b.reshape(1,-1)\n",
    "        epoch_to_convergence = 0 \n",
    "        while True:\n",
    "\n",
    "            wrong_classifications = 0\n",
    "            epoch_to_convergence+=1\n",
    "            for sample in X:\n",
    "                x = X[sample].reshape(1,-1)\n",
    "                y_in = np.dot(x , w) + b\n",
    "                y_in = 2 * softmax2(y_in)-1\n",
    "                result = self.activation(y_in)\n",
    "                target = y[sample[0]].reshape(1,-1)\n",
    "                difference = target - result\n",
    "                \n",
    "                if (difference!= 0).any():\n",
    "                    wrong_classifications += 1\n",
    "                    \n",
    "                    w = w + self.learning_rate * np.dot(x.T , target)\n",
    "                    b = b + self.learning_rate * target\n",
    "            if wrong_classifications == 0:\n",
    "                self.W = w\n",
    "                self.bias = b.T\n",
    "                return epoch_to_convergence\n",
    "    \n",
    "    \n",
    "    def predict(self ,test_sample):\n",
    "      x= test_sample.reshape(1,-1)\n",
    "      y_in = x.dot(self.W)\n",
    "      y_in = 2*softmax2(y_in)-1\n",
    "      y_out= self.activation(y_in)\n",
    "      return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(fpath,test = False) :\n",
    "    file_lists = os.listdir(fpath)\n",
    "    files_dic = dict()\n",
    "    for file in file_lists:\n",
    "        file_path = os.path.join(fpath , file)\n",
    "        with open(file_path,\"r\") as train_file:\n",
    "            content = train_file.read()\n",
    "            files_dic[file] = content\n",
    "    for key in files_dic:\n",
    "        if test:\n",
    "            files_dic[key] = files_dic[key].replace(\".\", \"0\").replace(\"#\", \"1\").replace(\"\\n\",\"\").replace('@','1').replace('o','0')\n",
    "        else:\n",
    "            files_dic[key] = files_dic[key].replace(\".\", \"0\").replace(\"#\", \"1\").replace(\"\\n\",\"\")\n",
    "        files_dic[key] = [int(x) for x in files_dic[key]]\n",
    "        files_dic[key] = np.array(files_dic[key])\n",
    "    return {a:files_dic[a] for a in files_dic}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b240db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = readfile(fpath = \"Characters-TrainSet/Characters-TrainSet\")\n",
    "test_data = readfile(fpath = \"Characters-TestSet/Characters-TestSet\",test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "labels['A']= np.array([1,-1,-1,-1,-1,-1,-1])\n",
    "labels['B']= np.array([-1,1,-1,-1,-1,-1,-1])\n",
    "labels['C']= np.array([-1,-1,1,-1,-1,-1,-1])\n",
    "\n",
    "labels['D']= np.array([-1,-1,-1,1,-1,-1,-1])\n",
    "labels['E']= np.array([-1,-1,-1,-1,1,-1,-1])\n",
    "labels['J']= np.array([-1,-1,-1,-1,-1,1,-1])\n",
    "labels['K']= np.array([-1,-1,-1,-1,-1,-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3163096",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = np.zeros([63 , 7])\n",
    "bias  = np.zeros(7)\n",
    "zero_weights = perceptron(weights1 , bias)\n",
    "zepochs  = zero_weights.fit(train_data,labels)\n",
    "zepochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7154f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dic1 = {}\n",
    "for item in test_data:\n",
    "    predict_dic1[item] = zero_weights.predict(test_data[item])\n",
    "wrong_predictions1 = np.sum([1 for prediction in predict_dic1 if( predict_dic1[prediction]!=labels[prediction[0]]).any()])\n",
    "error_rate1 = wrong_predictions1/len(predict_dic1)\n",
    "error_rate1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = np.random.uniform(low = -1.0 , high = 1.0 , size = (63,7))\n",
    "uniform_weights = perceptron(weights2 , bias)\n",
    "uniepochs  = uniform_weights.fit(train_data,labels)\n",
    "uniepochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dic2 = {}\n",
    "for item in test_data:\n",
    "    predict_dic2[item] =uniform_weights.predict(test_data[item])\n",
    "wrong_predictions2 = np.sum([1 for prediction in predict_dic2 if( predict_dic2[prediction]!=labels[prediction[0]]).any()])\n",
    "error_rate2 = wrong_predictions2/len(predict_dic2)\n",
    "error_rate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights3 = np.random.normal(loc = 0.0 , scale = 0.5 , size = (63,7))\n",
    "normal_weights = perceptron(weights3 , bias)\n",
    "norepochs  = normal_weights.fit(train_data,labels)\n",
    "norepochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfbbe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dic3 = {}\n",
    "for item in test_data:\n",
    "    predict_dic3[item] =normal_weights.predict(test_data[item])\n",
    "wrong_predictions3 = np.sum([1 for prediction in predict_dic3 if( predict_dic3[prediction]!=labels[prediction[0]]).any()])\n",
    "error_rate3 = wrong_predictions3/len(predict_dic3)\n",
    "error_rate3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db194c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_d = np.sqrt(2 / (63 + 7))\n",
    "weights4 = np.random.normal(loc = 0.0 , scale = std_d , size = (63,7))\n",
    "xavier_weights = perceptron(weights4 , bias)\n",
    "xepochs  = xavier_weights.fit(train_data,labels)\n",
    "xepochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dic4 = {}\n",
    "for item in test_data:\n",
    "    predict_dic4[item] =xavier_weights.predict(test_data[item])\n",
    "wrong_predictions4 = np.sum([1 for prediction in predict_dic4 if( predict_dic4[prediction]!=labels[prediction[0]]).any()])\n",
    "error_rate4 = wrong_predictions4/len(predict_dic4)\n",
    "error_rate4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "std_d1 = np.sqrt(2 / 63 )\n",
    "weights5 = np.random.normal(loc = 0.0 , scale = std_d1 , size = (63,7))\n",
    "kaiming_weights = perceptron(weights5 , bias)\n",
    "kepochs  = kaiming_weights.fit(train_data,labels)\n",
    "kepochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62446d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dic5 = {}\n",
    "for item in test_data:\n",
    "    predict_dic5[item] =xavier_weights.predict(test_data[item])\n",
    "wrong_predictions5 = np.sum([1 for prediction in predict_dic5 if( predict_dic5[prediction]!=labels[prediction[0]]).any()])\n",
    "error_rate5 = wrong_predictions5/len(predict_dic5)\n",
    "error_rate5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni = perceptron( None , bias)\n",
    "performance_uni = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    uni.set_weight (np.random.uniform(low = -1.0 , high = 1.0 , size = (63,7)))\n",
    "    e1 = uni.fit(train_data , labels)\n",
    "    predict_dic = {}\n",
    "    for item in test_data:\n",
    "        predict_dic[item] =uni.predict(test_data[item])\n",
    "    wrong_predictions = np.sum([1 for prediction in predict_dic if( predict_dic[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate = wrong_predictions/len(predict_dic)\n",
    "    performance_uni.loc[i] = [e1,error_rate]\n",
    "    \n",
    "performance_uni['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de55b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nor = perceptron( None , bias)\n",
    "performance_nor = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    nor.set_weight (np.random.normal(loc = 0.0 , scale = 0.5 , size = (63,7)))\n",
    "    e1 = nor.fit(train_data , labels)\n",
    "    predict_dic = {}\n",
    "    for item in test_data:\n",
    "        predict_dic[item] =nor.predict(test_data[item])\n",
    "    wrong_predictions = np.sum([1 for prediction in predict_dic if( predict_dic[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate = wrong_predictions/len(predict_dic)\n",
    "    performance_nor.loc[i] = [e1,error_rate]\n",
    "    \n",
    "performance_nor['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a517675",
   "metadata": {},
   "outputs": [],
   "source": [
    "xa = perceptron( None , bias)\n",
    "performance_xa = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    xa.set_weight (np.random.normal(loc = 0.0 , scale = np.sqrt(2 / (63 + 7)) , size = (63,7)))\n",
    "    e1 = xa.fit(train_data , labels)\n",
    "    predict_dic = {}\n",
    "    for item in test_data:\n",
    "        predict_dic[item] =xa.predict(test_data[item])\n",
    "    wrong_predictions = np.sum([1 for prediction in predict_dic if( predict_dic[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate = wrong_predictions/len(predict_dic)\n",
    "    performance_xa.loc[i] = [e1,error_rate]\n",
    "    \n",
    "performance_xa['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ke = perceptron( None , bias)\n",
    "performance_ke = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    ke.set_weight (np.random.normal(loc = 0.0 , scale = np.sqrt(2 / 63 ) , size = (63,7)))\n",
    "    e1 = ke.fit(train_data , labels)\n",
    "    predict_dic = {}\n",
    "    for item in test_data:\n",
    "        predict_dic[item] =ke.predict(test_data[item])\n",
    "    wrong_predictions = np.sum([1 for prediction in predict_dic if( predict_dic[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate = wrong_predictions/len(predict_dic)\n",
    "    performance_ke.loc[i] = [e1,error_rate]\n",
    "    \n",
    "performance_ke['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ebcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the performance\n",
    "print(performance_ke)\n",
    "print(performance_xa)\n",
    "print(performance_nor)\n",
    "print(performance_uni)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.normal(loc = 0.0 , scale = np.sqrt(2 / (63 + 7)) , size = (63,7))\n",
    "test = perceptron(weights  , bias)\n",
    "performance_theta = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in np.arange(0.1 , 1 , 0.05):\n",
    "    test.set_weight(weights)\n",
    "    test.set_theta(i)\n",
    "    e1 = test.fit(train_data , labels)\n",
    "    predict_dic = {}\n",
    "    for item in test_data:\n",
    "        predict_dic[item] =test.predict(test_data[item])\n",
    "    wrong_predictions = np.sum([1 for prediction in predict_dic if( predict_dic[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate = wrong_predictions/len(predict_dic)\n",
    "    performance_theta.loc[i] = [e1,error_rate]\n",
    "    \n",
    "performance_theta['error rate'].plot(title= 'Error rate vs  theta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c26ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "performance_LR = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "test.set_theta(0.3)\n",
    "for i in np.arange(0.1 , 1 , 0.1):\n",
    "    test.set_weight(weights)\n",
    "    test.set_theta(i)\n",
    "    e1 = test.fit(train_data , labels)\n",
    "    predict_dic = {}\n",
    "    for item in test_data:\n",
    "        predict_dic[item] =test.predict(test_data[item])\n",
    "    wrong_predictions = np.sum([1 for prediction in predict_dic if( predict_dic[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate = wrong_predictions/len(predict_dic)\n",
    "    performance_LR.loc[i] = [e1,error_rate]\n",
    "    \n",
    "performance_LR['error rate'].plot(title= 'Error rate vs  learning rate')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
