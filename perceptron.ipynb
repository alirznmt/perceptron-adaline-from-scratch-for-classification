{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b101f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import erf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax2(x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1)\n",
    "\n",
    "class perceptron():\n",
    "    def __init__(self ,W ,bias , learning_rate = 0.1 , theta = 0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.theta = theta\n",
    "        self.W = W\n",
    "        self.bias = bias \n",
    "    \n",
    "    def set_weight(self , weight):\n",
    "        self.W  = weight\n",
    "        \n",
    "    def set_theta(self ,theta):\n",
    "        self.theta = theta\n",
    "        \n",
    "    def set_learningrate(self , lr):\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "    def activation(self , Y):\n",
    "        res = np.ones_like(Y)\n",
    "        res[Y > self.theta] = 1  \n",
    "           \n",
    "        res[(Y > -self.theta) & (Y < self.theta)] = 0  \n",
    "             \n",
    "        res[Y < -self.theta] = -1\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self , X , y ):\n",
    "        w = self.W\n",
    "        b = self.bias \n",
    "        b = b.reshape(1,-1)\n",
    "        epoch_to_convergence = 0 \n",
    "        while True:\n",
    "\n",
    "            wrong_classifications = 0\n",
    "            epoch_to_convergence+=1\n",
    "            for sample in X:\n",
    "                x = X[sample].reshape(1,-1)\n",
    "                y_in = np.dot(x , w) + b\n",
    "                y_in = 2 * softmax2(y_in)-1\n",
    "                result = self.activation(y_in)\n",
    "                target = y[sample[0]].reshape(1,-1)\n",
    "                difference = target - result\n",
    "                \n",
    "                if (difference!= 0).any():\n",
    "                    wrong_classifications += 1\n",
    "                    \n",
    "                    w = w + self.learning_rate * np.dot(x.T , target)\n",
    "                    b = b + self.learning_rate * target\n",
    "            if wrong_classifications == 0:\n",
    "                self.W = w\n",
    "                self.bias = b.T\n",
    "                return epoch_to_convergence\n",
    "    \n",
    "    \n",
    "    def predict(self ,test_sample):\n",
    "      x= test_sample.reshape(1,-1)\n",
    "      y_in = x.dot(self.W)\n",
    "      y_in = 2*softmax2(y_in)-1\n",
    "      y_out= self.activation(y_in)\n",
    "      return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(fpath,test = False) :\n",
    "    file_lists = os.listdir(fpath)\n",
    "    files_dic = dict()\n",
    "    for file in file_lists:\n",
    "        file_path = os.path.join(fpath , file)\n",
    "        with open(file_path,\"r\") as train_file:\n",
    "            content = train_file.read()\n",
    "            files_dic[file] = content\n",
    "    for key in files_dic:\n",
    "        if test:\n",
    "            files_dic[key] = files_dic[key].replace(\".\", \"0\").replace(\"#\", \"1\").replace(\"\\n\",\"\").replace('@','1').replace('o','0')\n",
    "        else:\n",
    "            files_dic[key] = files_dic[key].replace(\".\", \"0\").replace(\"#\", \"1\").replace(\"\\n\",\"\")\n",
    "        files_dic[key] = [int(x) for x in files_dic[key]]\n",
    "        files_dic[key] = np.array(files_dic[key])\n",
    "    return {a:files_dic[a] for a in files_dic}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = readfile(fpath = \"Characters-TrainSet/Characters-TrainSet\")\n",
    "test_data = readfile(fpath = \"Characters-TestSet/Characters-TestSet\",test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f520725",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "labels['A']= np.array([1,-1,-1,-1,-1,-1,-1])\n",
    "labels['B']= np.array([-1,1,-1,-1,-1,-1,-1])\n",
    "labels['C']= np.array([-1,-1,1,-1,-1,-1,-1])\n",
    "\n",
    "labels['D']= np.array([-1,-1,-1,1,-1,-1,-1])\n",
    "labels['E']= np.array([-1,-1,-1,-1,1,-1,-1])\n",
    "labels['J']= np.array([-1,-1,-1,-1,-1,1,-1])\n",
    "labels['K']= np.array([-1,-1,-1,-1,-1,-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9388fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = np.zeros([63 , 7])\n",
    "bias  = np.zeros(7)\n",
    "zero_weights = perceptron(weights1 , bias)\n",
    "zepochs  = zero_weights.fit(train_data,labels)\n",
    "zepochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d621cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dic1 = {}\n",
    "for item in test_data:\n",
    "    predict_dic1[item] = zero_weights.predict(test_data[item])\n",
    "wrong_predictions1 = np.sum([1 for prediction in predict_dic1 if( predict_dic1[prediction]!=labels[prediction[0]]).any()])\n",
    "error_rate1 = wrong_predictions1/len(predict_dic1)\n",
    "error_rate1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62fa873",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = np.random.uniform(low = -1.0 , high = 1.0 , size = (63,7))\n",
    "uniform_weights = perceptron(weights2 , bias)\n",
    "uniepochs  = uniform_weights.fit(train_data,labels)\n",
    "uniepochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c2a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dic2 = {}\n",
    "for item in test_data:\n",
    "    predict_dic2[item] =uniform_weights.predict(test_data[item])\n",
    "wrong_predictions2 = np.sum([1 for prediction in predict_dic2 if( predict_dic2[prediction]!=labels[prediction[0]]).any()])\n",
    "error_rate2 = wrong_predictions2/len(predict_dic2)\n",
    "error_rate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b59aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights3 = np.random.normal(loc = 0.0 , scale = 0.5 , size = (63,7))\n",
    "normal_weights = perceptron(weights3 , bias)\n",
    "norepochs  = normal_weights.fit(train_data,labels)\n",
    "norepochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dic3 = {}\n",
    "for item in test_data:\n",
    "    predict_dic3[item] =normal_weights.predict(test_data[item])\n",
    "wrong_predictions3 = np.sum([1 for prediction in predict_dic3 if( predict_dic3[prediction]!=labels[prediction[0]]).any()])\n",
    "error_rate3 = wrong_predictions3/len(predict_dic3)\n",
    "error_rate3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_d = np.sqrt(2 / (63 + 7))\n",
    "weights4 = np.random.normal(loc = 0.0 , scale = std_d , size = (63,7))\n",
    "xavier_weights = perceptron(weights4 , bias)\n",
    "xepochs  = xavier_weights.fit(train_data,labels)\n",
    "xepochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dic4 = {}\n",
    "for item in test_data:\n",
    "    predict_dic4[item] =xavier_weights.predict(test_data[item])\n",
    "wrong_predictions4 = np.sum([1 for prediction in predict_dic4 if( predict_dic4[prediction]!=labels[prediction[0]]).any()])\n",
    "error_rate4 = wrong_predictions4/len(predict_dic4)\n",
    "error_rate4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "std_d1 = np.sqrt(2 / 63 )\n",
    "weights5 = np.random.normal(loc = 0.0 , scale = std_d1 , size = (63,7))\n",
    "kaiming_weights = perceptron(weights5 , bias)\n",
    "kepochs  = kaiming_weights.fit(train_data,labels)\n",
    "kepochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dic5 = {}\n",
    "for item in test_data:\n",
    "    predict_dic5[item] =xavier_weights.predict(test_data[item])\n",
    "wrong_predictions5 = np.sum([1 for prediction in predict_dic5 if( predict_dic5[prediction]!=labels[prediction[0]]).any()])\n",
    "error_rate5 = wrong_predictions5/len(predict_dic5)\n",
    "error_rate5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afeacf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni = perceptron( None , bias)\n",
    "performance_uni = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    uni.set_weight (np.random.uniform(low = -1.0 , high = 1.0 , size = (63,7)))\n",
    "    e1 = uni.fit(train_data , labels)\n",
    "    predict_dic = {}\n",
    "    for item in test_data:\n",
    "        predict_dic[item] =uni.predict(test_data[item])\n",
    "    wrong_predictions = np.sum([1 for prediction in predict_dic if( predict_dic[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate = wrong_predictions/len(predict_dic)\n",
    "    performance_uni.loc[i] = [e1,error_rate]\n",
    "    \n",
    "performance_uni['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06861c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nor = perceptron( None , bias)\n",
    "performance_nor = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    nor.set_weight (np.random.normal(loc = 0.0 , scale = 0.5 , size = (63,7)))\n",
    "    e1 = nor.fit(train_data , labels)\n",
    "    predict_dic = {}\n",
    "    for item in test_data:\n",
    "        predict_dic[item] =nor.predict(test_data[item])\n",
    "    wrong_predictions = np.sum([1 for prediction in predict_dic if( predict_dic[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate = wrong_predictions/len(predict_dic)\n",
    "    performance_nor.loc[i] = [e1,error_rate]\n",
    "    \n",
    "performance_nor['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce53679",
   "metadata": {},
   "outputs": [],
   "source": [
    "xa = perceptron( None , bias)\n",
    "performance_xa = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    xa.set_weight (np.random.normal(loc = 0.0 , scale = np.sqrt(2 / (63 + 7)) , size = (63,7)))\n",
    "    e1 = xa.fit(train_data , labels)\n",
    "    predict_dic = {}\n",
    "    for item in test_data:\n",
    "        predict_dic[item] =xa.predict(test_data[item])\n",
    "    wrong_predictions = np.sum([1 for prediction in predict_dic if( predict_dic[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate = wrong_predictions/len(predict_dic)\n",
    "    performance_xa.loc[i] = [e1,error_rate]\n",
    "    \n",
    "performance_xa['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ke = perceptron( None , bias)\n",
    "performance_ke = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    ke.set_weight (np.random.normal(loc = 0.0 , scale = np.sqrt(2 / 63 ) , size = (63,7)))\n",
    "    e1 = ke.fit(train_data , labels)\n",
    "    predict_dic = {}\n",
    "    for item in test_data:\n",
    "        predict_dic[item] =ke.predict(test_data[item])\n",
    "    wrong_predictions = np.sum([1 for prediction in predict_dic if( predict_dic[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate = wrong_predictions/len(predict_dic)\n",
    "    performance_ke.loc[i] = [e1,error_rate]\n",
    "    \n",
    "performance_ke['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06959156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the performance\n",
    "print(performance_ke)\n",
    "print(performance_xa)\n",
    "print(performance_nor)\n",
    "print(performance_uni)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0baa25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.normal(loc = 0.0 , scale = np.sqrt(2 / (63 + 7)) , size = (63,7))\n",
    "test = perceptron(weights  , bias)\n",
    "performance_theta = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in np.arange(0.1 , 1 , 0.05):\n",
    "    test.set_weight(weights)\n",
    "    test.set_theta(i)\n",
    "    e1 = test.fit(train_data , labels)\n",
    "    predict_dic = {}\n",
    "    for item in test_data:\n",
    "        predict_dic[item] =test.predict(test_data[item])\n",
    "    wrong_predictions = np.sum([1 for prediction in predict_dic if( predict_dic[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate = wrong_predictions/len(predict_dic)\n",
    "    performance_theta.loc[i] = [e1,error_rate]\n",
    "    \n",
    "performance_theta['error rate'].plot(title= 'Error rate vs  theta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "performance_LR = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "test.set_theta(0.3)\n",
    "for i in np.arange(0.1 , 1 , 0.1):\n",
    "    test.set_weight(weights)\n",
    "    test.set_theta(i)\n",
    "    e1 = test.fit(train_data , labels)\n",
    "    predict_dic = {}\n",
    "    for item in test_data:\n",
    "        predict_dic[item] =test.predict(test_data[item])\n",
    "    wrong_predictions = np.sum([1 for prediction in predict_dic if( predict_dic[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate = wrong_predictions/len(predict_dic)\n",
    "    performance_LR.loc[i] = [e1,error_rate]\n",
    "    \n",
    "performance_LR['error rate'].plot(title= 'Error rate vs  learning rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ce4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class adline():\n",
    "    def __init__(self ,W ,bias , learning_rate = 0.1 , theta = 0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.theta = theta\n",
    "        self.W = W\n",
    "        self.bias = bias \n",
    "    \n",
    "    def set_weight(self , weight):\n",
    "        self.W  = weight\n",
    "        \n",
    "    def set_theta(self ,theta):\n",
    "        self.theta = theta\n",
    "        \n",
    "    def set_learningrate(self , lr):\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "    def activation(self , Y):\n",
    "        res = np.ones_like(Y)\n",
    "        res[Y > self.theta] = 1  \n",
    "           \n",
    "        res[(Y > -self.theta) & (Y < self.theta)] = 0  \n",
    "             \n",
    "        res[Y < -self.theta] = -1\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self , X , y ):\n",
    "        w = self.W\n",
    "        b = self.bias \n",
    "        b = b.reshape(1,-1)\n",
    "        time_to_convergence = 0 \n",
    "        while True:\n",
    "            \n",
    "            wrong_classifications = 0\n",
    "            time_to_convergence+=1\n",
    "            for sample in X:\n",
    "                x = X[sample].reshape(1,-1)\n",
    "                y_in = np.dot(x , w) + b\n",
    "                y_in = 2 * softmax2(y_in)-1\n",
    "                result = self.activation(y_in)\n",
    "                target = y[sample[0]].reshape(1,-1)\n",
    "                difference = target - result\n",
    "                \n",
    "                \n",
    "                if (difference!= 0).any():\n",
    "                    wrong_classifications += 1\n",
    "                    delta_w=self.learning_rate*x.T.dot(target)\n",
    "                    w = w +  delta_w\n",
    "                    b = b + self.learning_rate * target\n",
    "            if wrong_classifications == 0:\n",
    "                self.W = w\n",
    "                self.bias = b.T\n",
    "                return time_to_convergence\n",
    "    \n",
    "    \n",
    "    def predict(self ,test_sample):\n",
    "      x= test_sample.reshape(1,-1)\n",
    "      y_in = x.dot(self.W)\n",
    "      y_in = 2*softmax2(y_in)-1\n",
    "      y_out= self.activation(y_in)\n",
    "      return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f22ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights5 = np.zeros([63,7])\n",
    "zero_weights2 = adline(weights5 , bias)\n",
    "zepochs2  = zero_weights2.fit(train_data,labels)\n",
    "zepochs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68084ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dic6 = {}\n",
    "for item in test_data:\n",
    "    predict_dic6[item] = zero_weights2.predict(test_data[item])\n",
    "wrong_predictions6 = np.sum([1 for prediction in predict_dic6 if( predict_dic6[prediction]!=labels[prediction[0]]).any()])\n",
    "error_rate6 = wrong_predictions6/len(predict_dic6)\n",
    "error_rate6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni2 = adline( None , bias)\n",
    "performance_uni2 = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    uni2.set_weight (np.random.uniform(low = -1.0 , high = 1.0 , size = (63,7)))\n",
    "    e2 = uni2.fit(train_data , labels)\n",
    "    predict_dic7 = {}\n",
    "    for item in test_data:\n",
    "        predict_dic7[item] =uni2.predict(test_data[item])\n",
    "    wrong_predictions7 = np.sum([1 for prediction in predict_dic7 if( predict_dic7[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate7 = wrong_predictions7/len(predict_dic7)\n",
    "    performance_uni2.loc[i] = [e2,error_rate7]\n",
    "    \n",
    "performance_uni2['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nor2 = adline( None , bias)\n",
    "performance_nor2 = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    nor2.set_weight (np.random.normal(loc = 0.0 , scale = 0.5 , size = (63,7)))\n",
    "    e2 = nor2.fit(train_data , labels)\n",
    "    predict_dic7 = {}\n",
    "    for item in test_data:\n",
    "        predict_dic7[item] =nor2.predict(test_data[item])\n",
    "    wrong_predictions7 = np.sum([1 for prediction in predict_dic7 if( predict_dic7[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate7 = wrong_predictions7/len(predict_dic7)\n",
    "    performance_nor2.loc[i] = [e2,error_rate7]\n",
    "    \n",
    "performance_nor['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xa2 = adline( None , bias)\n",
    "performance_xa2 = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    xa2.set_weight (np.random.normal(loc = 0.0 , scale = np.sqrt(2 / (63 + 7)) , size = (63,7)))\n",
    "    e2 = xa2.fit(train_data , labels)\n",
    "    predict_dic2 = {}\n",
    "    for item in test_data:\n",
    "        predict_dic7[item] =xa2.predict(test_data[item])\n",
    "    wrong_predictions7 = np.sum([1 for prediction in predict_dic7 if( predict_dic7[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate7 = wrong_predictions7/len(predict_dic7)\n",
    "    performance_xa2.loc[i] = [e2,error_rate7]\n",
    "    \n",
    "performance_xa2['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936792a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ke2 = adline( None , bias)\n",
    "performance_ke2 = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    ke2.set_weight (np.random.normal(loc = 0.0 , scale = np.sqrt(2 / 63 ) , size = (63,7)))\n",
    "    e2 = ke2.fit(train_data , labels)\n",
    "    predict_dic7 = {}\n",
    "    for item in test_data:\n",
    "        predict_dic7[item] =ke2.predict(test_data[item])\n",
    "    wrong_predictions7 = np.sum([1 for prediction in predict_dic7 if( predict_dic7[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate7 = wrong_predictions7/len(predict_dic7)\n",
    "    performance_ke2.loc[i] = [e2,error_rate7]\n",
    "    \n",
    "performance_ke2['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights7 = np.random.normal(loc = 0.0 , scale = np.sqrt(2 / (63 + 7)) , size = (63,7))\n",
    "test2 = adline(weights7  , bias)\n",
    "performance_theta2 = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in np.arange(0.1 , 1 , 0.05):\n",
    "    test2.set_weight(weights7)\n",
    "    test2.set_theta(i)\n",
    "    e2 = test2.fit(train_data , labels)\n",
    "    predict_dic7 = {}\n",
    "    for item in test_data:\n",
    "        predict_dic7[item] =test2.predict(test_data[item])\n",
    "    wrong_predictions7 = np.sum([1 for prediction in predict_dic if( predict_dic7[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate7 = wrong_predictions7/len(predict_dic7)\n",
    "    performance_theta2.loc[i] = [e2,error_rate7]\n",
    "    \n",
    "performance_theta2['error rate'].plot(title= 'Error rate vs  theta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df638661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "performance_LR2 = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "test2.set_theta(0.3)\n",
    "for i in np.arange(0.1 , 1 , 0.1):\n",
    "    test2.set_weight(weights7)\n",
    "    test2.set_theta(i)\n",
    "    e2 = test2.fit(train_data , labels)\n",
    "    predict_dic7 = {}\n",
    "    for item in test_data:\n",
    "        predict_dic7[item] =test2.predict(test_data[item])\n",
    "    wrong_predictions7 = np.sum([1 for prediction in predict_dic7 if( predict_dic7[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate7 = wrong_predictions7/len(predict_dic7)\n",
    "    performance_LR2.loc[i] = [e2,error_rate7]\n",
    "    \n",
    "performance_LR2['error rate'].plot(title= 'Error rate vs  learning rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c7b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e635a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best(df, target_df, method, initial_weight):\n",
    "    min_epoch_row = df.loc[df.loc[df['error rate'] == df['error rate'].min()]['epoch_to_converge'].idxmin()]\n",
    "    \n",
    "    new_row = {\n",
    "        'model': method,\n",
    "        'initial_weight': initial_weight,\n",
    "        'error_rate': min_epoch_row['error rate'],\n",
    "        'epochs': min_epoch_row['epoch_to_converge']\n",
    "    }\n",
    "    \n",
    "    target_df = target_df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    return target_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df = pd.DataFrame(columns = [\"model\" , 'initial_weight' , 'epochs' , 'error_rate' ])\n",
    "\n",
    "\n",
    "compare_df = find_best(performance_xa, compare_df, method=\"Perceptron\", initial_weight=\"Xavier\")\n",
    "compare_df = find_best(performance_ke, compare_df, method=\"Perceptron\", initial_weight=\"Kaiming\")\n",
    "compare_df = find_best(performance_nor, compare_df, method=\"Perceptron\", initial_weight=\"Random Normal\")\n",
    "compare_df = find_best(performance_xa2, compare_df, method=\"Adline\", initial_weight=\"Xavier\")\n",
    "compare_df = find_best(performance_ke2, compare_df, method=\"Adline\", initial_weight=\"Kaiming\")\n",
    "compare_df = find_best(performance_nor2, compare_df, method=\"Adline\", initial_weight=\"Random Normal\")\n",
    "compare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746bfc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function read file in projecton way\n",
    "def readfile2(fpath,test = False) :\n",
    "    file_lists = os.listdir(fpath)\n",
    "    files_dic = dict()\n",
    "    for file in file_lists:\n",
    "        file_path = os.path.join(fpath , file)\n",
    "        with open(file_path,\"r\") as train_file:\n",
    "            content = train_file.read()\n",
    "            files_dic[file] = content\n",
    "    for key in files_dic:\n",
    "        if test:\n",
    "            files_dic[key] = files_dic[key].replace(\".\", \"0\").replace(\"#\", \"1\").replace(\"\\n\",\"\").replace('@','1').replace('o','0')\n",
    "        else:\n",
    "            files_dic[key] = files_dic[key].replace(\".\", \"0\").replace(\"#\", \"1\").replace(\"\\n\",\"\")\n",
    "\n",
    "        files_dic[key] = [int(x) for x in files_dic[key]]\n",
    "        files_dic[key] = np.array(files_dic[key]).reshape(9 , 7)\n",
    "        files_dic[key] = np.concatenate([files_dic[key].sum(axis=1),files_dic[key].sum(axis=0)]) # add 1 that are in a row and column\n",
    "    return {a:files_dic[a] for a in files_dic}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2 = readfile2('C:/Users/ZETTA/Downloads/Characters-TrainSet/Characters-TrainSet')\n",
    "test_data2 = readfile2('C:/Users/ZETTA/Downloads/Characters-TestSet/Characters-TestSet',test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5609ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xavier weight\n",
    "xa3= adline( None , bias , learning_rate = 1)\n",
    "performance_xa3 = pd.DataFrame(columns = ['epoch_to_converge','error rate'])\n",
    "for i in range(50):\n",
    "    xa3.set_weight (np.random.random([16,7]))\n",
    "    e2 = xa3.fit(train_data2 , labels)\n",
    "    predict_dic7 = {}\n",
    "    for item in test_data:\n",
    "        predict_dic7[item] =xa3.predict(test_data2[item])\n",
    "    wrong_predictions7 = np.sum([1 for prediction in predict_dic7 if( predict_dic7[prediction]!=labels[prediction[0]]).any()])\n",
    "    error_rate7 = wrong_predictions7/len(predict_dic7)\n",
    "    performance_xa3.loc[i] = [e2,error_rate7]\n",
    "\n",
    "performance_xa3['error rate'].plot(title= 'Error rate vs initial weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86e4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
